{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDDM analysis of EMBARC PRT data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mlm2/anaconda/lib/python3.5/site-packages/IPython/parallel.py:13: ShimWarning: The `IPython.parallel` package has been deprecated. You should import from ipyparallel instead.\n",
      "  \"You should import from ipyparallel instead.\", ShimWarning)\n",
      "/Users/mlm2/anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:185: RRuntimeWarning: Loading required package: Matrix\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/mlm2/anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:185: RRuntimeWarning: Loading required package: lme4\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/mlm2/anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:185: RRuntimeWarning: \n",
      "Attaching package: ‘lmerTest’\n",
      "\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/mlm2/anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:185: RRuntimeWarning: The following object is masked from ‘package:lme4’:\n",
      "\n",
      "    lmer\n",
      "\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/mlm2/anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:185: RRuntimeWarning: The following object is masked from ‘package:stats’:\n",
      "\n",
      "    step\n",
      "\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/mlm2/anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:185: RRuntimeWarning: Loading required package: estimability\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/mlm2/anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:185: RRuntimeWarning: \n",
      "Attaching package: ‘lsmeans’\n",
      "\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/mlm2/anaconda/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:185: RRuntimeWarning: The following object is masked from ‘package:lmerTest’:\n",
      "\n",
      "    lsmeans\n",
      "\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['sjmisc', 'sjPlot', 'lsmeans', 'estimability', 'lmerTest', 'lme4',\n",
       "       'Matrix', 'ez', 'tools', 'stats', 'graphics', 'grDevices', 'utils',\n",
       "       'datasets', 'methods', 'base'], \n",
       "      dtype='<U12')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime, mmap,os,re, sys, hddm, decimal \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from patsy import dmatrix\n",
    "# below makes graphs open in the nb instead of in a separate window\n",
    "% matplotlib inline \n",
    "%reload_ext rpy2.ipython\n",
    "\n",
    "# R\n",
    "%R library(ez)\n",
    "%R library(lmerTest)\n",
    "%R library(lsmeans)\n",
    "%R lsm.options(disable.pbkrtest=TRUE)\n",
    "#%R library(mutoss)\n",
    "%R library(sjPlot)\n",
    "%R library(sjmisc)\n",
    "#%R library(coefplot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uname = !whoami\n",
    "uname = uname[0]\n",
    "path2analysis = '/Users/' + uname + '/Work/Expts/EMBARC/' # also where git repo lives\n",
    "today = datetime.datetime.today().strftime(\"%m_%d_%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /Users/mlm2/Work/Expts/EMBARC/Data/PRT/Massachusetts/MGTemp/: File exists\n",
      "mkdir: /Users/mlm2/Work/Expts/EMBARC/Data/PRT/Michigan/UMTemp/: File exists\n",
      "mkdir: /Users/mlm2/Work/Expts/EMBARC/Data/PRT/New_York/CUTemp/: File exists\n",
      "usage: cp [-R [-H | -L | -P]] [-fi | -n] [-apvX] source_file target_file\n",
      "       cp [-R [-H | -L | -P]] [-fi | -n] [-apvX] source_file ... target_directory\n",
      "mkdir: /Users/mlm2/Work/Expts/EMBARC/Data/PRT/Texas/TXTemp/: File exists\n"
     ]
    }
   ],
   "source": [
    "for val in [('Massachusetts','MG'),('Michigan','UM'),('New_York','CU'),('Texas','TX')]:\n",
    "    state, abrev = val\n",
    "    base_dir = '/Users/' + uname + '/Work/Expts/EMBARC/Data/PRT/' + state + '/'\n",
    "    temp_dir = base_dir + abrev + 'Temp/'\n",
    "    old_files = base_dir + 'embarc_CU_' + abrev + '0*/done/sigdet_output*out'\n",
    "    flist = !ls {old_files}\n",
    "    \n",
    "    !mkdir {temp_dir}\n",
    "    \n",
    "    for old_fname in flist:\n",
    "        if abrev != 'CU':\n",
    "            sub = old_fname.split('_')[2]\n",
    "        else:\n",
    "            sub = old_fname.split('_')[3]\n",
    "        new_file = sub + '_out.txt'\n",
    "        new_fname = temp_dir + new_file\n",
    "        \n",
    "        !cp {old_fname} {new_fname} # Done this way you don't get error messages when there's no old file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SigDetParse(fname):\n",
    "    '''Open the file, use regex to pull out key vars, output a clean df.'''\n",
    "    \n",
    "    df = []\n",
    "    d = {}\n",
    "    ct = 0\n",
    "    \n",
    "    # Begin reading in data from the line including 'reward_due', as that's unlikely to occur anywhere earlier\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            if 'reward_due' in line:\n",
    "                for line in f:\n",
    "                    d[ct] = {'trial':line.split('\\t')[0],\n",
    "                             'length':line.split('\\t')[1],\n",
    "                             'time':line.split('\\t')[2], # tempting to change to RT but keep same for backwards compat\n",
    "                             'key_press':line.split('\\t')[3],\n",
    "                             'correct':line.split('\\t')[4],\n",
    "                             'did_reward':line.split('\\t')[5],\n",
    "                             'reward_due':line.split('\\t')[6],\n",
    "                             'rich_due':line.split('\\t')[7],\n",
    "                             'lean_due':line.split('\\t')[8],\n",
    "                             'outlier':line.split('\\t')[9].strip('\\n')}\n",
    "                    ct = ct + 1\n",
    "    df = pd.DataFrame.from_dict(d,orient='index')\n",
    "    \n",
    "\n",
    "    for val in [('rich_key','Rich key:'),('lean_key','Lean key:'), ('subject','Subject ID:'), ('date','Date:'), \n",
    "                ('bias','Bias:'), ('rich_stim','Rich stimulus:'), ('lean_stim','Lean stimulus:')]:\n",
    "        \n",
    "        var_name, pattern = val\n",
    "        df[var_name] = np.nan # Need some default values b/c some subjects have missing data        \n",
    "\n",
    "        if var_name != 'date':\n",
    "            var_def = re.compile(pattern + '[\\s]+([\\w]+)')\n",
    "        else:\n",
    "            var_def = re.compile(pattern + '[\\s]+([\\d]+/[\\d]+/[\\d]+)')\n",
    "        \n",
    "        with open(fname) as f:\n",
    "            for line in f:\n",
    "                var_match = var_def.search(line)\n",
    "                if var_match:\n",
    "                    result = var_match.group(1)\n",
    "                    #checks if the subject ID is weird, prints it, and fixes it. For this data, all 3 digit sub ids\n",
    "                    #are fixed by adding a 0 to the beginning. \n",
    "                    if var_name =='subject' and len(result) !=4:\n",
    "                        print ('Check sub ID: ' + fname)\n",
    "                        if len(result) == 3:\n",
    "                            df[var_name]='0'+ result\n",
    "                        if len(result) == 1:\n",
    "                            df[var_name] ='000' + result\n",
    "                    else:\n",
    "                        df[var_name] = result\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check sub ID: /Users/mlm2/Work/Expts/EMBARC/Data/PRT/Massachusetts/MGTemp/MG0020MGBP1R1_out.txt\n",
      "Check sub ID: /Users/mlm2/Work/Expts/EMBARC/Data/PRT/Massachusetts/MGTemp/MG0025MGBP2R1_out.txt\n",
      "Check sub ID: /Users/mlm2/Work/Expts/EMBARC/Data/PRT/Massachusetts/MGTemp/MG0027MGBP1R1_out.txt\n",
      "Check sub ID: /Users/mlm2/Work/Expts/EMBARC/Data/PRT/Massachusetts/MGTemp/MG0039MGBP2R1_out.txt\n",
      "Empty file: /Users/mlm2/Work/Expts/EMBARC/Data/PRT/Massachusetts/MGTemp/MG0270MGBP1R1_out.txt\n",
      "Check sub ID: /Users/mlm2/Work/Expts/EMBARC/Data/PRT/Michigan/UMTemp/UM0001UMBP1R1_out.txt\n",
      "Empty file: /Users/mlm2/Work/Expts/EMBARC/Data/PRT/Texas/TXTemp/TX0038MGBP3R1_out.txt\n",
      "Empty file: /Users/mlm2/Work/Expts/EMBARC/Data/PRT/Texas/TXTemp/TX0198TXBP2R1_out.txt\n"
     ]
    }
   ],
   "source": [
    "# Now you can just iterate over the files in each dir and use the code above to read each file into a ginorm df\n",
    "today = datetime.datetime.today().strftime(\"%m_%d_%Y\")\n",
    "\n",
    "dfs = []\n",
    "base_dir = '/Users/' + uname + '/Work/Expts/EMBARC/Data/PRT/'\n",
    "for val in [('Massachusetts','MG'),('Michigan','UM'),('New_York','CU'),('Texas','TX')]:\n",
    "    state, abrev = val\n",
    "    state_dir = base_dir + state + '/' + abrev + 'Temp'\n",
    "    flist = !ls {state_dir}\n",
    "    \n",
    "    for fname in flist:\n",
    "        curr_path = state_dir + '/' + fname \n",
    "        statinfo = os.stat(curr_path) # checking for empty files b/c I found one . . .\n",
    "        if statinfo.st_size == 0:\n",
    "            #to make it compatible with 2 & 3\n",
    "            print ('Empty file: '+ curr_path)\n",
    "        else:\n",
    "            df = SigDetParse(curr_path)\n",
    "            df['site'] = abrev\n",
    "            df['ProjectSpecificID'] = df['site'] + df['subject']\n",
    "            dfs.append(df)\n",
    "out = pd.concat(dfs)\n",
    "out = out[['ProjectSpecificID','site','subject','date','bias','rich_stim','lean_stim','rich_key','lean_key',\n",
    "           'trial','length','time','key_press','correct','did_reward','reward_due','rich_due','lean_due','outlier']]\n",
    "out.to_csv(path2analysis + 'Data/embarc_PRT_all_trials_all_sessions_' + today + ' .csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProjectSpecificID</th>\n",
       "      <th>site</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>bias</th>\n",
       "      <th>rich_stim</th>\n",
       "      <th>lean_stim</th>\n",
       "      <th>rich_key</th>\n",
       "      <th>lean_key</th>\n",
       "      <th>trial</th>\n",
       "      <th>length</th>\n",
       "      <th>time</th>\n",
       "      <th>key_press</th>\n",
       "      <th>correct</th>\n",
       "      <th>did_reward</th>\n",
       "      <th>reward_due</th>\n",
       "      <th>rich_due</th>\n",
       "      <th>lean_due</th>\n",
       "      <th>outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MG0001</td>\n",
       "      <td>MG</td>\n",
       "      <td>0001</td>\n",
       "      <td>12/14/2011</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "      <td>long</td>\n",
       "      <td>c</td>\n",
       "      <td>m</td>\n",
       "      <td>1</td>\n",
       "      <td>short</td>\n",
       "      <td>683</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MG0001</td>\n",
       "      <td>MG</td>\n",
       "      <td>0001</td>\n",
       "      <td>12/14/2011</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "      <td>long</td>\n",
       "      <td>c</td>\n",
       "      <td>m</td>\n",
       "      <td>2</td>\n",
       "      <td>long</td>\n",
       "      <td>1307</td>\n",
       "      <td>c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MG0001</td>\n",
       "      <td>MG</td>\n",
       "      <td>0001</td>\n",
       "      <td>12/14/2011</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "      <td>long</td>\n",
       "      <td>c</td>\n",
       "      <td>m</td>\n",
       "      <td>3</td>\n",
       "      <td>short</td>\n",
       "      <td>577</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MG0001</td>\n",
       "      <td>MG</td>\n",
       "      <td>0001</td>\n",
       "      <td>12/14/2011</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "      <td>long</td>\n",
       "      <td>c</td>\n",
       "      <td>m</td>\n",
       "      <td>4</td>\n",
       "      <td>long</td>\n",
       "      <td>677</td>\n",
       "      <td>m</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MG0001</td>\n",
       "      <td>MG</td>\n",
       "      <td>0001</td>\n",
       "      <td>12/14/2011</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "      <td>long</td>\n",
       "      <td>c</td>\n",
       "      <td>m</td>\n",
       "      <td>5</td>\n",
       "      <td>short</td>\n",
       "      <td>724</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ProjectSpecificID site subject        date   bias rich_stim lean_stim  \\\n",
       "0            MG0001   MG    0001  12/14/2011  short     short      long   \n",
       "1            MG0001   MG    0001  12/14/2011  short     short      long   \n",
       "2            MG0001   MG    0001  12/14/2011  short     short      long   \n",
       "3            MG0001   MG    0001  12/14/2011  short     short      long   \n",
       "4            MG0001   MG    0001  12/14/2011  short     short      long   \n",
       "\n",
       "  rich_key lean_key trial length  time key_press correct did_reward  \\\n",
       "0        c        m     1  short   683         c       1          0   \n",
       "1        c        m     2   long  1307         c       0          0   \n",
       "2        c        m     3  short   577         c       1          1   \n",
       "3        c        m     4   long   677         m       1          1   \n",
       "4        c        m     5  short   724         c       1          0   \n",
       "\n",
       "  reward_due rich_due lean_due outlier  \n",
       "0          0        0        0       0  \n",
       "1          1        0        1       1  \n",
       "2          1        0        1       0  \n",
       "3          1        0        0       0  \n",
       "4          0        0        0       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProjectSpecificID</th>\n",
       "      <th>site</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>bias</th>\n",
       "      <th>rich_stim</th>\n",
       "      <th>lean_stim</th>\n",
       "      <th>rich_key</th>\n",
       "      <th>lean_key</th>\n",
       "      <th>trial</th>\n",
       "      <th>length</th>\n",
       "      <th>time</th>\n",
       "      <th>key_press</th>\n",
       "      <th>correct</th>\n",
       "      <th>did_reward</th>\n",
       "      <th>reward_due</th>\n",
       "      <th>rich_due</th>\n",
       "      <th>lean_due</th>\n",
       "      <th>outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>TX0204</td>\n",
       "      <td>TX</td>\n",
       "      <td>0204</td>\n",
       "      <td>10/12/2015</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "      <td>long</td>\n",
       "      <td>c</td>\n",
       "      <td>m</td>\n",
       "      <td>196</td>\n",
       "      <td>short</td>\n",
       "      <td>686</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>TX0204</td>\n",
       "      <td>TX</td>\n",
       "      <td>0204</td>\n",
       "      <td>10/12/2015</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "      <td>long</td>\n",
       "      <td>c</td>\n",
       "      <td>m</td>\n",
       "      <td>197</td>\n",
       "      <td>long</td>\n",
       "      <td>365</td>\n",
       "      <td>m</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>TX0204</td>\n",
       "      <td>TX</td>\n",
       "      <td>0204</td>\n",
       "      <td>10/12/2015</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "      <td>long</td>\n",
       "      <td>c</td>\n",
       "      <td>m</td>\n",
       "      <td>198</td>\n",
       "      <td>long</td>\n",
       "      <td>846</td>\n",
       "      <td>m</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>TX0204</td>\n",
       "      <td>TX</td>\n",
       "      <td>0204</td>\n",
       "      <td>10/12/2015</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "      <td>long</td>\n",
       "      <td>c</td>\n",
       "      <td>m</td>\n",
       "      <td>199</td>\n",
       "      <td>short</td>\n",
       "      <td>414</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>TX0204</td>\n",
       "      <td>TX</td>\n",
       "      <td>0204</td>\n",
       "      <td>10/12/2015</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "      <td>long</td>\n",
       "      <td>c</td>\n",
       "      <td>m</td>\n",
       "      <td>200</td>\n",
       "      <td>short</td>\n",
       "      <td>1083</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProjectSpecificID site subject        date   bias rich_stim lean_stim  \\\n",
       "195            TX0204   TX    0204  10/12/2015  short     short      long   \n",
       "196            TX0204   TX    0204  10/12/2015  short     short      long   \n",
       "197            TX0204   TX    0204  10/12/2015  short     short      long   \n",
       "198            TX0204   TX    0204  10/12/2015  short     short      long   \n",
       "199            TX0204   TX    0204  10/12/2015  short     short      long   \n",
       "\n",
       "    rich_key lean_key trial length  time key_press correct did_reward  \\\n",
       "195        c        m   196  short   686         m       0          0   \n",
       "196        c        m   197   long   365         m       1          1   \n",
       "197        c        m   198   long   846         m       1          0   \n",
       "198        c        m   199  short   414         c       1          1   \n",
       "199        c        m   200  short  1083         c       1          0   \n",
       "\n",
       "    reward_due rich_due lean_due outlier  \n",
       "195          1        1        0       0  \n",
       "196          1        1        0       0  \n",
       "197          0        1        0       0  \n",
       "198          1        0        0       0  \n",
       "199          0        0        0       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0001', '0002', '0003', '0004', '0005', '0006', '0007', '0008',\n",
       "       '0018', '0020', '0021', '0025', '0026', '0027', '0028', '0030',\n",
       "       '0032', '0039', '0040', '0051', '0060', '0064', '0066', '0069',\n",
       "       '0070', '0074', '0076', '0081', '0086', '0101', '0104', '0106',\n",
       "       '0112', '0116', '0117', '0120', '0125', '0126', '0135', '0137',\n",
       "       '0138', '0142', '0152', '0155', '0157', '0158', '0161', '0164',\n",
       "       '0168', '0172', '0180', '0182', '0185', '0187', '0198', '0202',\n",
       "       '0206', '0207', '0209', '0213', '0214', '0218', '0220', '0222',\n",
       "       '0228', '0230', '0231', '0238', '0239', '0242', '0243', '0246',\n",
       "       '0248', '0251', '0252', '0253', '0256', '0257', '0259', '0261',\n",
       "       '0269', '0270', '0009', '0011', '0012', '0014', '0015', '0016',\n",
       "       '0017', '0023', '0024', '0029', '0031', '0033', '0034', '0035',\n",
       "       '0036', '0037', '0038', '0042', '0046', '0047', '0048', '0049',\n",
       "       '0050', '0052', '0056', '0058', '0065', '0073', '0077', '0078',\n",
       "       '0079', '0080', '0082', '0083', '0085', '0088', '0089', '0090',\n",
       "       '0091', '0092', '0093', '0096', '0097', '0100', '0102', '0103',\n",
       "       '0107', '0108', '0110', '0111', '0113', '0114', '0115', '0118',\n",
       "       '0119', '0121', '0010', '0013', '0019', '0022', '0041', '0053',\n",
       "       '0055', '0057', '0059', '0061', '0062', '0063', '0067', '0068',\n",
       "       '0071', '0072', '0075', '0084', '0087', '0094', '0095', '0099',\n",
       "       '0105', '0127', '0128', '0129', '0130', '0131', '0132', '0133',\n",
       "       '0134', '0043', '0045', '0122', '0123', '0124', '0136', '0139',\n",
       "       '0140', '0141', '0145', '0147', '0148', '0149', '0151', '0153',\n",
       "       '0156', '0159', '0162', '0165', '0169', '0173', '0174', '0175',\n",
       "       '0177', '0178', '0179', '0184', '0188', '0189', '0193', '0194',\n",
       "       '0195', '0204'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that change fixed the wacky numbers, looks like it did\n",
    "out.subject.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mlm2/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (3,104,109,113,117,118,119,122,123,127,133,134,135,136,138,141,142,219,221,227,229,231,232,233,234,235,236,237,238,239,240,241,242,243,244,250,251,252,253,258,260,262,264,266,268,276,280,283,285,287,290,293,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,336,338,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,392,396,408,410,411,417,418,419,421,422,423,460,462,464,466,468,470,472,474,476,478,480,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,590,800,801,812,813,814,823,861) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#Combines the group data (groups.csv) with the task data, is really big, prob best to not work with\n",
    "#Excel won't even open this\n",
    "df_task = out\n",
    "df_group = []\n",
    "df_all = []\n",
    "\n",
    "df_group = pd.read_csv('Data/groups.csv',encoding=\"latin-1\")\n",
    "df_all = df_task.merge(df_group, how='outer')\n",
    "df_all = df_all[pd.notnull(df_all['ProjectSpecificID'])]\n",
    "df_all.to_csv('Data/embarc_combined_' + today +'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ProjectSpecificID',\n",
       " 'site',\n",
       " 'subject',\n",
       " 'date',\n",
       " 'bias',\n",
       " 'rich_stim',\n",
       " 'lean_stim',\n",
       " 'rich_key',\n",
       " 'lean_key',\n",
       " 'trial',\n",
       " 'length',\n",
       " 'time',\n",
       " 'key_press',\n",
       " 'correct',\n",
       " 'did_reward',\n",
       " 'reward_due',\n",
       " 'rich_due',\n",
       " 'lean_due',\n",
       " 'outlier',\n",
       " 'sample',\n",
       " 'feasibility',\n",
       " 'trainingset',\n",
       " 'aaq_1',\n",
       " 'aaq_2',\n",
       " 'aaq_3',\n",
       " 'aaq_4',\n",
       " 'aaq_5a',\n",
       " 'aaq_5b',\n",
       " 'aaq_5c',\n",
       " 'aaq_5d',\n",
       " 'aaq_5e',\n",
       " 'aaq_5f',\n",
       " 'aaq_5g',\n",
       " 'aaq_5h',\n",
       " 'aaq_5i',\n",
       " 'aaq_5j',\n",
       " 'aaq_5k',\n",
       " 'aaq_5l',\n",
       " 'aaq_5m',\n",
       " 'aaq_6',\n",
       " 'aaq_7',\n",
       " 'aaq_score_result',\n",
       " 'asrm_4',\n",
       " 'asrm_5',\n",
       " 'asrm_3',\n",
       " 'asrm_2',\n",
       " 'asrm_1',\n",
       " 'asrm_score2',\n",
       " 'atrq_01',\n",
       " 'atrq_02ada1',\n",
       " 'atrq_02ada2',\n",
       " 'atrq_02ana1',\n",
       " 'atrq_02ana2',\n",
       " 'atrq_02ase2',\n",
       " 'atrq_02ase1',\n",
       " 'atrq_02end2',\n",
       " 'atrq_02end1',\n",
       " 'atrq_02lud1',\n",
       " 'atrq_02lud2',\n",
       " 'atrq_02nor1',\n",
       " 'atrq_02nor2',\n",
       " 'atrq_02pam1',\n",
       " 'atrq_02pam2',\n",
       " 'atrq_02sin1',\n",
       " 'atrq_02sin2',\n",
       " 'atrq_02sur1',\n",
       " 'atrq_02sur2',\n",
       " 'atrq_02tof1',\n",
       " 'atrq_02tof2',\n",
       " 'atrq_02viv1',\n",
       " 'atrq_02viv2',\n",
       " 'atrq_02mar1',\n",
       " 'atrq_02mar2',\n",
       " 'atrq_02nar1',\n",
       " 'atrq_02nar2',\n",
       " 'atrq_02par1',\n",
       " 'atrq_02par2',\n",
       " 'atrq_02ems1',\n",
       " 'atrq_02ems2',\n",
       " 'atrq_02luv1',\n",
       " 'atrq_02luv2',\n",
       " 'atrq_02pax1',\n",
       " 'atrq_02pax2',\n",
       " 'atrq_02pro1',\n",
       " 'atrq_02pro2',\n",
       " 'atrq_02zol1',\n",
       " 'atrq_02zol2',\n",
       " 'atrq_02cel1',\n",
       " 'atrq_02cel2',\n",
       " 'atrq_02lex1',\n",
       " 'atrq_02lex2',\n",
       " 'atrq_02eff1',\n",
       " 'atrq_02eff2',\n",
       " 'atrq_02cym2',\n",
       " 'atrq_02cym1',\n",
       " 'atrq_02priq1',\n",
       " 'atrq_02priq2',\n",
       " 'atrq_02des1',\n",
       " 'atrq_02des2',\n",
       " 'atrq_02ser1',\n",
       " 'atrq_02ser2',\n",
       " 'atrq_02wel1',\n",
       " 'atrq_02wel2',\n",
       " 'atrq_02rem2',\n",
       " 'atrq_02rem1',\n",
       " 'atrq_02ect',\n",
       " 'atrq_04',\n",
       " 'atrq_02pax_cr',\n",
       " 'atrq_02pax_cr_dose',\n",
       " 'atrq_02eff_xr',\n",
       " 'atrq_02eff_xr_dose',\n",
       " 'atrq_valdox_1',\n",
       " 'atrq_valdox_dose',\n",
       " 'atrq_norval_1',\n",
       " 'atrq_norval_dose',\n",
       " 'atrq_viibryd_1',\n",
       " 'atrq_viibryd_dose',\n",
       " 'atrq_savella_1',\n",
       " 'atrq_savella_dose',\n",
       " 'biospc_6',\n",
       " 'biospc_8',\n",
       " 'biospc_9',\n",
       " 'biospc_05',\n",
       " 'number_tubes',\n",
       " 'biospc_plasma_trayid_ut',\n",
       " 'biospc_plasma_trayid_rt',\n",
       " 'biospc_plasma_shipDateUT',\n",
       " 'biospc_plasma_shipdatert',\n",
       " 'biospc_time',\n",
       " 'biospc_minutes',\n",
       " 'biospc_hours',\n",
       " 'biospc_05r',\n",
       " 'biospc_04r',\n",
       " 'biospc_03r',\n",
       " 'biospc_01r',\n",
       " 'biospc_rutgers_id',\n",
       " 'biospc2_washout',\n",
       " 'biospc2_bloodCollectDate',\n",
       " 'biospc2_bloodcollect_hr',\n",
       " 'biospc2_bloodcollect_min',\n",
       " 'biospc2_cellLineDate',\n",
       " 'biospc2_rutgers_id',\n",
       " 'biospc2_yellow_tubes',\n",
       " 'biospc2_purple_tubes',\n",
       " 'biospc2_paxgene_tubes',\n",
       " 'biospc2_blood_shipDateRT',\n",
       " 'biospc2_plasma_hour',\n",
       " 'biospc2_sum_tubes',\n",
       " 'biospc2_plasma_min',\n",
       " 'biospc2_num_aliquotfrzn',\n",
       " 'biospc2_add_aliquotfrzn',\n",
       " 'biospc2_plasma_shipDateRT',\n",
       " 'biospc2_plasma_shipDateUT',\n",
       " 'biospc2_blood_receiveDateRT',\n",
       " 'biospc2_plasma_trayID_RT',\n",
       " 'biospc2_pkaliquot_num',\n",
       " 'biospc2_plasma_initials',\n",
       " 'biospc2_pkaliquot_shipdate',\n",
       " 'biospc2_pkaliquot_id',\n",
       " 'biospc2_plasma_trayID_UT',\n",
       " 'biospc2_memo',\n",
       " 'cast_01',\n",
       " 'cast_02',\n",
       " 'cast_03',\n",
       " 'cast_04',\n",
       " 'cast_05',\n",
       " 'cast_06',\n",
       " 'cast_07',\n",
       " 'cast_08',\n",
       " 'cast_09',\n",
       " 'cast_10',\n",
       " 'cast_11',\n",
       " 'cast_12',\n",
       " 'cast_13',\n",
       " 'cast_14',\n",
       " 'cast_15',\n",
       " 'cast_16',\n",
       " 'cast_17',\n",
       " 'cast_score_total',\n",
       " 'cgi_01',\n",
       " 'ctq_01',\n",
       " 'ctq_02',\n",
       " 'ctq_03',\n",
       " 'ctq_04',\n",
       " 'ctq_05',\n",
       " 'ctq_06',\n",
       " 'ctq_07',\n",
       " 'ctq_08',\n",
       " 'ctq_09',\n",
       " 'ctq_10',\n",
       " 'ctq_11',\n",
       " 'ctq_12',\n",
       " 'ctq_13',\n",
       " 'ctq_14',\n",
       " 'ctq_15',\n",
       " 'ctq_16',\n",
       " 'ctq_17',\n",
       " 'ctq_18',\n",
       " 'ctq_19',\n",
       " 'ctq_20',\n",
       " 'ctq_21',\n",
       " 'ctq_22',\n",
       " 'ctq_23',\n",
       " 'ctq_24',\n",
       " 'ctq_25',\n",
       " 'ctq_26',\n",
       " 'ctq_27',\n",
       " 'ctq_28',\n",
       " 'ctqscore_ea',\n",
       " 'ctqscore_en',\n",
       " 'ctqscore_pa',\n",
       " 'ctqscore_pn',\n",
       " 'ctqscore_sa',\n",
       " 'ctqscore_val',\n",
       " 'Interval',\n",
       " 'chrt_01',\n",
       " 'chrt_02',\n",
       " 'chrt_03',\n",
       " 'chrt_04',\n",
       " 'chrt_05',\n",
       " 'chrt_06',\n",
       " 'chrt_07',\n",
       " 'chrt_08',\n",
       " 'chrt_09',\n",
       " 'chrt_10',\n",
       " 'chrt_11',\n",
       " 'chrt_12',\n",
       " 'chrt_13',\n",
       " 'chrt_14',\n",
       " 'chrt_15',\n",
       " 'chrt_16',\n",
       " 'chrtp_propensity_score',\n",
       " 'chrtp_risk_score',\n",
       " 'chf_01',\n",
       " 'chf_02',\n",
       " 'chf_03',\n",
       " 'chf_04',\n",
       " 'chf_05',\n",
       " 'chf_06',\n",
       " 'chf_07',\n",
       " 'chf_08',\n",
       " 'chf_09',\n",
       " 'chf_10',\n",
       " 'chf_11',\n",
       " 'chf_12',\n",
       " 'chf_12a',\n",
       " 'chf_13',\n",
       " 'chf_13a',\n",
       " 'chf_14',\n",
       " 'chf_14a',\n",
       " 'chf_wash_fluox',\n",
       " 'chf_wash_benzo',\n",
       " 'chf_wash_other',\n",
       " 'chf_date1',\n",
       " 'chf_date2',\n",
       " 'chf_date3',\n",
       " 'chf_date4',\n",
       " 'chf_date5',\n",
       " 'chf_method1',\n",
       " 'chf_method2',\n",
       " 'chf_method3',\n",
       " 'chf_method4',\n",
       " 'chf_method5',\n",
       " 'chf_hospital1',\n",
       " 'chf_hospital2',\n",
       " 'chf_hospital3',\n",
       " 'chf_hospital4',\n",
       " 'chf_hospital5',\n",
       " 'chf_outcome1',\n",
       " 'chf_outcome2',\n",
       " 'chf_outcome3',\n",
       " 'chf_outcome4',\n",
       " 'chf_outcome5',\n",
       " 'chf_rate_suicide_lifetime',\n",
       " 'chf_num_suicide_attempts',\n",
       " 'cssrs_base_01',\n",
       " 'cssrs_base_01a',\n",
       " 'cssrs_base_02',\n",
       " 'cssrs_base_02a',\n",
       " 'cssrs_base_03',\n",
       " 'cssrs_base_03a',\n",
       " 'cssrs_base_04',\n",
       " 'cssrs_base_04a',\n",
       " 'cssrs_base_05',\n",
       " 'cssrs_base_05a',\n",
       " 'cssrs_base_06',\n",
       " 'cssrs_base_06a',\n",
       " 'cssrs_base_07',\n",
       " 'cssrs_base_08',\n",
       " 'cssrs_base_09',\n",
       " 'cssrs_base_10',\n",
       " 'cssrs_base_11',\n",
       " 'cssrs_base_12',\n",
       " 'cssrs_base_12a',\n",
       " 'cssrs_base_12b',\n",
       " 'cssrs_base_12c',\n",
       " 'cssrs_base_13',\n",
       " 'cssrs_base_13a',\n",
       " 'cssrs_base_13b',\n",
       " 'cssrs_base_14',\n",
       " 'cssrs_base_14a',\n",
       " 'cssrs_base_14b',\n",
       " 'cssrs_base_15',\n",
       " 'cssrs_base_15a',\n",
       " 'cssrs_base_16',\n",
       " 'cssrs_base_16a',\n",
       " 'cssrs_base_17a',\n",
       " 'cssrs_base_18a',\n",
       " 'cssrs_base_16b',\n",
       " 'cssrs_base_17b',\n",
       " 'cssrs_base_18b',\n",
       " 'cssrs_base_16c',\n",
       " 'cssrs_base_17c',\n",
       " 'cssrs_base_18c',\n",
       " 'cssrs_base_intesity_score',\n",
       " 'chrt_que2',\n",
       " 'chrt_que3',\n",
       " 'chrt_que4',\n",
       " 'chrt_que5',\n",
       " 'chrt_que6',\n",
       " 'chrt_que7',\n",
       " 'chrt_que8',\n",
       " 'chrt_que9',\n",
       " 'chrt_que_1a',\n",
       " 'chrtc_que2a',\n",
       " 'conmed_01',\n",
       " 'conmed_02',\n",
       " 'conmed_03',\n",
       " 'conmed_04',\n",
       " 'conmed_05',\n",
       " 'conmed_06',\n",
       " 'conmed_07',\n",
       " 'conmed_08',\n",
       " 'conmed_09',\n",
       " 'conmed_10',\n",
       " 'conmed_11',\n",
       " 'conmed_12',\n",
       " 'conmed_13',\n",
       " 'conmed_14',\n",
       " 'conmed_15',\n",
       " 'conmed_16',\n",
       " 'conmed_17',\n",
       " 'conmed_18',\n",
       " 'conmed_19',\n",
       " 'conmed_20',\n",
       " 'conmed_21',\n",
       " 'conmed_22',\n",
       " 'conmed_23',\n",
       " 'conmed_24',\n",
       " 'conmed_25',\n",
       " 'conmed_26',\n",
       " 'conmed_27',\n",
       " 'conmed_28',\n",
       " 'conmed_29',\n",
       " 'conmed_30',\n",
       " 'conmed_start',\n",
       " 'SiteName',\n",
       " 'chronicity',\n",
       " 'severity1',\n",
       " 'sex',\n",
       " 'hispa',\n",
       " 'race',\n",
       " 'age_evaluation',\n",
       " 'demo_maritial_status',\n",
       " 'demo_hous_numbr',\n",
       " 'demo_employ_status',\n",
       " 'demo_educa_years',\n",
       " 'demo_educa_status',\n",
       " 'demo_ssn_status',\n",
       " 'demo_monthly_income',\n",
       " 'demo_income_que',\n",
       " 'ehi_result',\n",
       " 'qids_eval_total',\n",
       " 'pv_HCtravel_site',\n",
       " 'ehi_01',\n",
       " 'ehi_02',\n",
       " 'ehi_03',\n",
       " 'ehi_04',\n",
       " 'ehi_05',\n",
       " 'ehi_06',\n",
       " 'ehi_07',\n",
       " 'ehi_08',\n",
       " 'ehi_09',\n",
       " 'ehi_10',\n",
       " 'ehi_11',\n",
       " 'ehi_12',\n",
       " 'ehi_result.1',\n",
       " 'eeg_112',\n",
       " 'eeg_113',\n",
       " 'eeg_114',\n",
       " 'eeg_115',\n",
       " 'eeg_121',\n",
       " 'eeg_10',\n",
       " 'eeg_11',\n",
       " 'eeg_12',\n",
       " 'eeg_13',\n",
       " 'eeg_116',\n",
       " 'eeg_108',\n",
       " 'eeg_109',\n",
       " 'eeg_110',\n",
       " 'eeg_111',\n",
       " 'eeg_117',\n",
       " 'eeg_118',\n",
       " 'eeg_119',\n",
       " 'eeg_120',\n",
       " 'partici_questions',\n",
       " 'eeg_speilberger',\n",
       " 'qids_eeg_score',\n",
       " 'eeg_vams_check',\n",
       " 'eeg_fager',\n",
       " 'eeg_exp_initials',\n",
       " 'eeg_nasion',\n",
       " 'eeg_ear_ear',\n",
       " 'eeg_head_circum',\n",
       " 'eeg_cap_size',\n",
       " 'beh_word_fluency',\n",
       " 'beh_flanker',\n",
       " 'beh_a_not_b',\n",
       " 'beh_12',\n",
       " 'beh_13',\n",
       " 'wfss_7',\n",
       " 'wfss_8',\n",
       " 'wfss_9',\n",
       " 'choice_4',\n",
       " 'wfss_10',\n",
       " 'wfss_11',\n",
       " 'eeg_138',\n",
       " 'beh_14',\n",
       " 'eeg_data_tran_date',\n",
       " 'pheno_data_tran_date',\n",
       " 'eeg_scan_complete',\n",
       " 'beh_scan_complete',\n",
       " 'eeg_mri_complete',\n",
       " 'site_name_eeg',\n",
       " 'sample_group_eeg',\n",
       " 'target_washout_eeg',\n",
       " 'eeg_date',\n",
       " 'partic_name',\n",
       " 'partic_age',\n",
       " 'partic_gender',\n",
       " 'site_eeg',\n",
       " 'beh_text_field',\n",
       " 'ftnd_8',\n",
       " 'ftnd_9',\n",
       " 'ftnd_10',\n",
       " 'ftnd_11',\n",
       " 'ftnd_12',\n",
       " 'ftnd_13',\n",
       " 'ftnd_7',\n",
       " 'ftnd_score_total',\n",
       " 'mri_scan_complete',\n",
       " 'fhs_01_parent',\n",
       " 'fhs_01_sibs',\n",
       " 'fhs_01_kids',\n",
       " 'fhs_02',\n",
       " 'fhs_02_par',\n",
       " 'fhs_02_sibs',\n",
       " 'fhs_02_kids',\n",
       " 'fhs_03',\n",
       " 'fhs_03_par',\n",
       " 'fhs_03_sibs',\n",
       " 'fhs_03_kids',\n",
       " 'fhs_04',\n",
       " 'fhs_04_par',\n",
       " 'fhs_04_sibs',\n",
       " 'fhs_04_kids',\n",
       " 'fhs_05',\n",
       " 'fhs_05_par',\n",
       " 'fhs_05_sibs',\n",
       " 'fhs_05_kids',\n",
       " 'fhs_06',\n",
       " 'fhs_06_par',\n",
       " 'fhs_06_sibs',\n",
       " 'fhs_06_kids',\n",
       " 'fhs_07',\n",
       " 'fhs_07_par',\n",
       " 'fhs_07_sibs',\n",
       " 'fhs_07_kids',\n",
       " 'mhf_01',\n",
       " 'mhf_02',\n",
       " 'mhf_03',\n",
       " 'mhf_04',\n",
       " 'mhf_05',\n",
       " 'mhf_06',\n",
       " 'mhf_07',\n",
       " 'mhf_08',\n",
       " 'mhf_09',\n",
       " 'mhf_10',\n",
       " 'mhf_11',\n",
       " 'mhf_12',\n",
       " 'mhf_13',\n",
       " 'mhf_14',\n",
       " 'mhf_15',\n",
       " 'mhf_16',\n",
       " 'mhf_17',\n",
       " 'mhf_18',\n",
       " 'mhf_19',\n",
       " 'mhf_20',\n",
       " 'mhf_21',\n",
       " 'mhf_22',\n",
       " 'mhf_23',\n",
       " 'mhf_24',\n",
       " 'sfi_a',\n",
       " 'sfi_b',\n",
       " 'sfi_c',\n",
       " 'sfi_d',\n",
       " 'sfi_e',\n",
       " 'sfi_f',\n",
       " 'Interval.1',\n",
       " 'masq2_01',\n",
       " 'masq2_02',\n",
       " 'masq2_03',\n",
       " 'masq2_04',\n",
       " 'masq2_05',\n",
       " 'masq2_06',\n",
       " 'masq2_07',\n",
       " 'masq2_08',\n",
       " 'masq2_09',\n",
       " 'masq2_10',\n",
       " 'masq2_11',\n",
       " 'masq2_12',\n",
       " 'masq2_13',\n",
       " 'masq2_14',\n",
       " 'masq2_15',\n",
       " 'masq2_16',\n",
       " 'masq2_17',\n",
       " 'masq2_18',\n",
       " 'masq2_19',\n",
       " 'masq2_20',\n",
       " 'masq2_21',\n",
       " 'masq2_22',\n",
       " 'masq2_23',\n",
       " 'masq2_24',\n",
       " 'masq2_25',\n",
       " 'masq2_26',\n",
       " 'masq2_27',\n",
       " 'masq2_28',\n",
       " 'masq2_29',\n",
       " 'masq2_30',\n",
       " 'masq2_score_gd',\n",
       " 'masq2_score_ad',\n",
       " 'masq2_score_aa',\n",
       " 'q1a1',\n",
       " 'q1a2',\n",
       " 'q1a3',\n",
       " 'q1a4',\n",
       " 'q1a5',\n",
       " 'q1a6',\n",
       " 'q1a7',\n",
       " 'q1a8',\n",
       " 'q1a9',\n",
       " 'q1a10',\n",
       " 'q1a11',\n",
       " 'q1a12',\n",
       " 'q1a13',\n",
       " 'mdqscore_total',\n",
       " 'mri_ef_metal_check',\n",
       " 'vat_test',\n",
       " 'mri_ef_shaps',\n",
       " 'mri_ef_masdq',\n",
       " 'mri_ef_spiel1',\n",
       " 'mri_ef_seq_sel1',\n",
       " 'mri_ef_seq_sel10',\n",
       " 'mri_ef_seq_sel11',\n",
       " 'mri_ef_seq_sel12',\n",
       " 'mri_ef_seq_sel2',\n",
       " 'mri_ef_seq_sel3',\n",
       " 'mri_ef_seq_sel4',\n",
       " 'mri_ef_seq_sel5',\n",
       " 'mri_ef_seq_sel6',\n",
       " 'mri_ef_seq_sel7',\n",
       " 'mri_ef_seq_sel8',\n",
       " 'mri_ef_seq_sel9',\n",
       " 'mri_ef_seq_sel15',\n",
       " 'mri_ef_seq_sel14',\n",
       " 'mri_ef_seq_sel13',\n",
       " 'mri_ef_seq_num_atmp2',\n",
       " 'mri_ef_seq_num_atmp3',\n",
       " 'mri_ef_seq_num_atmp1',\n",
       " 'mri_ef_seq_num_atmp4',\n",
       " 'mri_ef_seq_num_atmp5',\n",
       " 'mri_ef_seq_num_atmp6',\n",
       " 'mri_ef_seq_num_atmp7',\n",
       " 'mri_ef_seq_num_atmp8',\n",
       " 'mri_ef_seq_num_atmp9',\n",
       " 'mri_ef_seq_num_atmp10',\n",
       " 'mri_ef_seq_num_atmp11',\n",
       " 'mri_ef_seq_num_atmp12',\n",
       " 'mri_ef_seq_num_atmp13',\n",
       " 'mri_ef_seq_num_atmp14',\n",
       " 'mri_ef_seq_num_atmp15',\n",
       " 'mri_ef_seq_com1',\n",
       " 'mri_ef_seq_com2',\n",
       " 'mri_ef_seq_com3',\n",
       " 'mri_ef_seq_com4',\n",
       " 'mri_ef_seq_com5',\n",
       " 'mri_ef130',\n",
       " 'mri_ef131',\n",
       " 'mri_ef132',\n",
       " 'mri_ef133',\n",
       " 'mri_ef134',\n",
       " 'mri_ef177',\n",
       " 'mri_ef178',\n",
       " 'mri_ef179',\n",
       " 'mri_ef180',\n",
       " 'mri_ef191',\n",
       " 'mri_ef_ra1',\n",
       " 'mri_ef_coord',\n",
       " 'mri_ef_data_date',\n",
       " 'mri_ef140',\n",
       " 'mri_ef144',\n",
       " 'mri_ef145',\n",
       " 'mri_ef146',\n",
       " 'mri_ef147',\n",
       " 'mri_ef148',\n",
       " 'mri_ef149',\n",
       " 'mri_ef150',\n",
       " 'mri_ef151',\n",
       " 'mri_ef152',\n",
       " 'mri_ef153',\n",
       " 'mri_ef181',\n",
       " 'mri_ef182',\n",
       " 'mri_ef183',\n",
       " 'mri_ef184',\n",
       " 'mri_ef192',\n",
       " 'mri_ef154',\n",
       " 'mri_ef155',\n",
       " 'mri_ef156',\n",
       " 'mri_ef157',\n",
       " 'mri_ef158',\n",
       " 'mri_ef159',\n",
       " 'mri_ef160',\n",
       " 'mri_ef162',\n",
       " 'mri_ef161',\n",
       " 'mri_ef163',\n",
       " 'mri_ef185',\n",
       " 'mri_ef186',\n",
       " 'mri_ef187',\n",
       " 'mri_ef188',\n",
       " 'mri_ef194',\n",
       " 'mri_ef193',\n",
       " 'mri_ef_pracstroop_sa_runs',\n",
       " 'mri_ef_pracstroop_sa_perct',\n",
       " 'mri_ef_pracstroop_in_runs',\n",
       " 'mri_ef_pracstroop_in_perct',\n",
       " 'mri_ef_reward_runs',\n",
       " 'mri_ef_reward_perct',\n",
       " 'mri_ef_reward_in_runs',\n",
       " 'mri_ef_reward_in_perct',\n",
       " 'mri_ef_emostrop_scanr_run',\n",
       " 'mri_ef_emostrop_scanr_perct',\n",
       " 'mri_scan_complete.1',\n",
       " 'neo2_01',\n",
       " 'neo2_02',\n",
       " 'neo2_03',\n",
       " 'neo2_04',\n",
       " 'neo2_05',\n",
       " 'neo2_06',\n",
       " 'neo2_07',\n",
       " 'neo2_08',\n",
       " 'neo2_9a',\n",
       " 'neo2_10',\n",
       " 'neo2_11',\n",
       " 'neo2_12',\n",
       " 'neo2_13',\n",
       " 'neo2_14',\n",
       " 'neo2_15',\n",
       " 'neo2_16',\n",
       " 'neo2_17',\n",
       " 'neo2_18',\n",
       " 'neo2_19',\n",
       " 'neo2_20',\n",
       " 'neo2_21',\n",
       " 'neo2_22',\n",
       " 'neo2_23',\n",
       " 'neo2_24',\n",
       " 'neo2_25',\n",
       " 'neo2_26',\n",
       " 'neo2_27',\n",
       " 'neo2_28',\n",
       " 'neo2_29',\n",
       " 'neo2_30',\n",
       " 'neo2_31',\n",
       " 'neo2_32',\n",
       " 'neo2_33',\n",
       " 'neo2_34',\n",
       " 'neo2_35',\n",
       " 'neo2_36',\n",
       " 'neo2_37',\n",
       " 'neo2_38',\n",
       " 'neo2_39',\n",
       " 'neo2_40',\n",
       " 'neo2_41',\n",
       " 'neo2_42',\n",
       " 'neo2_43',\n",
       " 'neo2_44',\n",
       " 'neo2_45',\n",
       " 'neo2_46',\n",
       " 'neo2_47',\n",
       " 'neo2_48',\n",
       " 'neo2_49',\n",
       " 'neo2_50',\n",
       " 'neo2_51',\n",
       " 'neo2_52',\n",
       " 'neo2_53',\n",
       " 'neo2_54',\n",
       " 'neo2_55',\n",
       " 'neo2_56',\n",
       " 'neo2_57',\n",
       " 'neo2_58',\n",
       " 'neo2_59',\n",
       " 'neo2_60',\n",
       " 'neo2_score_ne',\n",
       " 'neo2_score_ex',\n",
       " 'neo2_score_op',\n",
       " 'neo2_score_ag',\n",
       " 'neo2_score_co',\n",
       " 'scidad_01_dx',\n",
       " 'scidad_02_dx2',\n",
       " 'scidad_02_dx3',\n",
       " 'scidad_03_anxdis',\n",
       " 'scidad_03_dx',\n",
       " 'scidad_03_dx1',\n",
       " 'scid_ad_dm1',\n",
       " 'scid_ad1',\n",
       " 'scid_ad10',\n",
       " 'scid_ad10_a',\n",
       " 'scid_ad10_a1',\n",
       " 'scid_ad11',\n",
       " 'scid_ad11_a',\n",
       " 'scid_ad11_a1',\n",
       " 'scid_ad12',\n",
       " 'scid_ad13',\n",
       " 'scid_ad13_a',\n",
       " 'scid_ad13_a1',\n",
       " 'scid_ad14',\n",
       " 'scid_ad14_a',\n",
       " 'scid_ad14_a1',\n",
       " 'scid_ad15',\n",
       " 'scid_ad15_a',\n",
       " 'scid_ad15_a1',\n",
       " 'scid_ad15_a2',\n",
       " 'scid_ad15_a3',\n",
       " 'scid_ad16',\n",
       " 'scid_ad2',\n",
       " 'scid_ad3',\n",
       " 'scid_ad4',\n",
       " 'scid_ad5',\n",
       " 'scid_ad6',\n",
       " 'scid_ad7',\n",
       " 'scid_ad8',\n",
       " 'scid_ad9',\n",
       " 'scid_ad9_a',\n",
       " 'scid_ad9_a1',\n",
       " 'mdd1',\n",
       " 'mdd2',\n",
       " 'mdd7',\n",
       " 'mdd8',\n",
       " 'bpi1',\n",
       " 'bpii1',\n",
       " 'bpo1',\n",
       " 'Psycho_symp_LT',\n",
       " 'Psycho_symp_Cur',\n",
       " 'pan2_dx',\n",
       " 'pan1_dx',\n",
       " 'pan3_dx',\n",
       " 'agor_dx',\n",
       " 'agor_cur',\n",
       " 'ocd_dx',\n",
       " 'ocd_cur',\n",
       " 'pho_cur',\n",
       " 'pho_dx',\n",
       " 'spho_dx',\n",
       " 'ptsd_dx',\n",
       " 'ptsd_cur',\n",
       " 'GAD_cur',\n",
       " 'anx_GMC_dx',\n",
       " 'anx_GMC_cur',\n",
       " 'anx_GMC_specifier',\n",
       " 'anx_sub_specifier',\n",
       " 'anx_sub_cur',\n",
       " 'anx_sub_dx',\n",
       " 'anx_sub_specify',\n",
       " 'anx_NOS_cur',\n",
       " 'anx_NOS_dx',\n",
       " 'alc_dx',\n",
       " 'Alc_Cur',\n",
       " 'sed_cur',\n",
       " 'sed_dx',\n",
       " 'can_dx',\n",
       " 'can_cur',\n",
       " 'stim_dx',\n",
       " 'stim_cur',\n",
       " 'opi_cur',\n",
       " 'opi_dx',\n",
       " 'coc_cur',\n",
       " 'coc_dx',\n",
       " 'hal_cur',\n",
       " 'hal_dx',\n",
       " 'poly_cur',\n",
       " 'poly_dx',\n",
       " 'oth_dx',\n",
       " 'oth_cur',\n",
       " 'Other_AxisI_dx',\n",
       " 'Other_AxisI_cur',\n",
       " 'ano_dx',\n",
       " 'ano_cur',\n",
       " 'bul_cur',\n",
       " 'bul_dx',\n",
       " 'scid_abuse_depen_alcho',\n",
       " 'scid_abuse_depen_cocaine',\n",
       " 'scid_abuse_depen_opiate',\n",
       " 'scid_abuse_depen_other',\n",
       " 'scid_abuse_depen_pcp',\n",
       " 'scid_abuse_depen_polydrug',\n",
       " 'scid_abuse_depen_cannabis',\n",
       " 'scid_abuse_depen_stimulant',\n",
       " 'strf_01',\n",
       " 'strf_01a',\n",
       " 'strf_15',\n",
       " 'strf_02',\n",
       " 'current_pregnancy_nurse',\n",
       " 'strf_hypertension',\n",
       " 'strf_03',\n",
       " 'strf_04',\n",
       " 'strf_05',\n",
       " 'strf_06',\n",
       " 'strf_07',\n",
       " 'strf_08',\n",
       " 'strf_09',\n",
       " 'strf_10',\n",
       " 'strf_11',\n",
       " 'strf_11a',\n",
       " 'strf_12',\n",
       " 'strf_13',\n",
       " 'strf_14',\n",
       " 'strf_cholesterol_hdl',\n",
       " 'strf_cholesterol_ldl',\n",
       " 'strf_cholesterol_total',\n",
       " 'strf_cholesterol_triglycerides',\n",
       " 'strf_crp',\n",
       " 'strf_crp_lessthan_1',\n",
       " 'strf_fasting',\n",
       " 'strf_glucose',\n",
       " 'strf_tsh',\n",
       " 'strf_waist',\n",
       " 'scq_01',\n",
       " 'scq_02',\n",
       " 'scq_03',\n",
       " 'scq_04',\n",
       " 'scq_05',\n",
       " 'scq_06',\n",
       " 'scq_07',\n",
       " 'scq_08',\n",
       " 'scq_09',\n",
       " 'scq_10',\n",
       " 'scq_11',\n",
       " 'scq_12',\n",
       " 'scq_13',\n",
       " 'scq_14',\n",
       " 'scq_15',\n",
       " 'scq_16_a',\n",
       " 'scq_total_score',\n",
       " 'shaps_1',\n",
       " 'shaps_2',\n",
       " 'shaps_3',\n",
       " 'shaps_4',\n",
       " 'shaps_5',\n",
       " 'shaps_6',\n",
       " 'shaps_7',\n",
       " 'shaps_8',\n",
       " 'shaps_9',\n",
       " 'shaps_10',\n",
       " 'shaps_11',\n",
       " 'shaps_12',\n",
       " 'shaps_13',\n",
       " 'shaps_14',\n",
       " 'shaps_total_dichotomous',\n",
       " 'shaps_total_continuous',\n",
       " 'IntervalName',\n",
       " 'sas_sr_01',\n",
       " 'sas_sr_02',\n",
       " 'sas_sr_03',\n",
       " 'sas_sr_04',\n",
       " 'sas_sr_05',\n",
       " 'sas_sr_06',\n",
       " 'sas_sr_07',\n",
       " 'sas_sr_08',\n",
       " 'sas_sr_09',\n",
       " 'sas_sr_10',\n",
       " 'sas_sr_11',\n",
       " 'sas_sr_12',\n",
       " 'sas_sr_13',\n",
       " 'sas_sr_14',\n",
       " 'sas_sr_15',\n",
       " 'sas_sr_16',\n",
       " 'sas_sr_17',\n",
       " 'sas_sr_18',\n",
       " 'sas_sr_19',\n",
       " 'sas_sr_20',\n",
       " 'sas_sr_21',\n",
       " 'sas_sr_22',\n",
       " 'sas_sr_23',\n",
       " 'sas_sr_24',\n",
       " 'sas_sr_1a',\n",
       " 'sas_sr_2a',\n",
       " 'sas_sr_3a',\n",
       " 'sas_sr_3c',\n",
       " 'sas_sr_4a',\n",
       " 'sas_sr_5a',\n",
       " 'sas_sr_7b',\n",
       " 'sas_sr_8b',\n",
       " 'sas_sr_9b',\n",
       " 'sas_overall_factor',\n",
       " 'sas_overall_numques',\n",
       " 'sas_overall_mean',\n",
       " 'stai_10',\n",
       " 'stai_11',\n",
       " 'stai_12',\n",
       " 'stai_13',\n",
       " 'stai_14',\n",
       " 'stai_15',\n",
       " 'stai_16',\n",
       " 'stai_17',\n",
       " 'stai_33',\n",
       " 'stai_34',\n",
       " 'stai_35',\n",
       " 'stai_36',\n",
       " 'stai_37',\n",
       " 'stai_38',\n",
       " 'stai_39',\n",
       " 'stai_40',\n",
       " 'stai_41',\n",
       " 'stai_42',\n",
       " 'stai_43',\n",
       " 'stai_44',\n",
       " 'stai_post_final_score',\n",
       " 'stai_2',\n",
       " 'stai_3',\n",
       " 'stai_4',\n",
       " 'stai_5',\n",
       " 'stai_6',\n",
       " 'stai_7',\n",
       " 'stai_8',\n",
       " 'stai_9',\n",
       " 'stai_21',\n",
       " 'stai_22',\n",
       " 'stai_23',\n",
       " 'stai_24',\n",
       " 'stai_25',\n",
       " 'stai_26',\n",
       " 'stai_27',\n",
       " 'stai_28',\n",
       " 'stai_29',\n",
       " 'stai_30',\n",
       " 'stai_31',\n",
       " 'stai_32',\n",
       " 'stai_pre_final_score',\n",
       " 'stai_18',\n",
       " 'stai_19',\n",
       " 'stai_20',\n",
       " 'stai_45',\n",
       " 'stai_46',\n",
       " 'stai_47',\n",
       " 'stai_48',\n",
       " 'stai_49',\n",
       " 'stai_50',\n",
       " 'stai_51',\n",
       " 'stai_52',\n",
       " 'stai_53',\n",
       " 'stai_54',\n",
       " 'stai_55',\n",
       " 'stai_56',\n",
       " 'stai_57',\n",
       " 'stai_58',\n",
       " 'stai_59',\n",
       " 'stai_60',\n",
       " 'stai_61',\n",
       " 'stai_eeg_final_score',\n",
       " 'sapas_01',\n",
       " 'sapas_02',\n",
       " 'sapas_03',\n",
       " 'sapas_04',\n",
       " 'sapas_05',\n",
       " 'sapas_06',\n",
       " 'sapas_07',\n",
       " 'sapas_08',\n",
       " 'sapas_score',\n",
       " 'IntervalName.1',\n",
       " 'VAMS_soc1',\n",
       " 'vams_rel1',\n",
       " 'vams_wit1',\n",
       " 'vams_hap1',\n",
       " 'VAS1',\n",
       " 'wasi_vocab_raw',\n",
       " 'wasi_vocab_tscore',\n",
       " 'wasi_matrix_raw',\n",
       " 'wasi_matrix_tscore',\n",
       " 'hamd_01',\n",
       " 'hamd_02',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_compact = []\n",
    "df_compact = df_all[['site', 'subject', 'date', 'bias', 'rich_stim', 'lean_stim', 'rich_key', 'lean_key', 'trial', \n",
    "                    'length', 'length', 'time', 'key_press', 'correct', 'did_reward', 'reward_due', 'rich_due', 'lean_due',\n",
    "                    'outlier', 'sample', 'feasibility', 'trainingset', 'sex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    144210\n",
      "1      4030\n",
      "Name: outlier, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# number of outlier trials (1.0) in ALL sessions\n",
    "print (df_all[\"outlier\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MOVE THIS DOWN!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "# Much better way to get the first session by merging with standard analysis data and taking the first session. I need \n",
    "# to move the order of the cells around to make it more intuitive, this should come after the standard analysis stuff.\n",
    "\n",
    "df_reuse = df_all.merge(standardfinal)\n",
    "df_reuse = df_reuse[pd.notnull(df_reuse['ProjectSpecificID'])]\n",
    "\n",
    "# renaming time & correct; the hddm package requires specific column names\n",
    "df_reuse.rename(columns={'time':'rt', 'correct': 'response', 'ProjectSpecificID': 'subj_idx'}, inplace = True)\n",
    "\n",
    "df_reuse.loc[df_reuse['feasibility'] == 1, 'feasibility'] = 'MDD'\n",
    "df_reuse.loc[df_reuse['feasibility'] == 3, 'feasibility'] = 'MDD'\n",
    "df_reuse.loc[df_reuse['feasibility'] == 5, 'feasibility'] = 'MDD'\n",
    "df_reuse.loc[df_reuse['feasibility'] == 4, 'feasibility'] = 'CTL'\n",
    "\n",
    "# number of outlier trials and non-outlier trials for the FIRST session\n",
    "print(df_reuse[\"outlier\"].value_counts())\n",
    "\n",
    "# don't include the outlier trials\n",
    "df_reuse = df_reuse[df_reuse.outlier != 1]\n",
    "\n",
    "# save it as embarc_compact...\n",
    "df_reuse.to_csv('embarc_compact_first_sess' + today + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dupesdropped = df_reuse.drop_duplicates\n",
    "#%Rpush df_dupesdropped\n",
    "#%R cat <- ezPlot(data = df_dupesdropped, dv = RB_adjst_all_Blk2_minus_Blk1, between = (feasibility, site), x= feasibility, wid=ProjectSpecificID)\n",
    "#%R print(cat)\n",
    "#%R ay <- ezANOVA(data = df_mg, dv = RB_adjst_all_Blk2_minus_Blk1, between = feasibility, wid=ProjectSpecificID)\n",
    "#%R print(ay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dan note:** So far so good! Please count the number of outlier trials and the % of trials that are outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "#### Massachusetts:\n",
    "MG0081 & MG0164 had two sessions labled '1.' Since both sessions were complete, the ones with the earlier dates were used. Christian included MG020, which was originally coded as only having a session two; this was probably a typo and I included it. After these corrections everything matched, except that Christian's list included MG0164 twice (so our n for MDDs is one less than his). \n",
    "\n",
    "Our descriptives are a little different than Christian's, even in the HCs. \n",
    "#### New York: \n",
    "CU0017, CU0018, and CU0019 had two sessions labled '1.' Since both sessions were complete and had all data I just took the ones with the earlier dates. Christian also excluded CU0047, so I removed them from ours. It's unclear from the data why they were excluded. \n",
    "\n",
    "Our descriptives look the same as Christian's.\n",
    "#### Michigan:\n",
    "UM0012 & UM0017 had two sessions labled '1.' Since both sessions were complete I took the first of the two for each. UM0016 had three lines of the exact same session 1 data, so I only included the first line. Christian also excluded UM0102, so I removed them from ours. It's unclear from the data why they were excluded.\n",
    "\n",
    "Our descriptives look the same as Christian's except our MDD n is 51 and his is 52. I would guess that UM0102 was not included in the list of subIDs that were included, but was included when doing the descriptives. We should decide whether or not to keep this person. \n",
    "#### Texas:\n",
    "Everything lined up with no alterations!\n",
    "\n",
    "Our descriptives look the same as Christian's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dan note:** Above looks good, for us you might also check whether including the second set of session 1 data for people with duplicates makes any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------DELETE SOON-------------------\n",
    "\n",
    "def replacetabs(state):\n",
    "    '''Open the file, replace the tabs with spaces, save the new doc as a csv.'''\n",
    "    !mkdir {'/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/'+ state +'/Summaries/Temp'}\n",
    "    statesum = '/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/'+ state +'/Summaries/embarc*/*txt'\n",
    "    folders = !ls -d {statesum}\n",
    "    newfolders = '/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/'+ state +'/Summaries/Temp/'\n",
    "    newfolders2 = '/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/'+ state +'/Summaries/Temp'\n",
    "    dfall = []\n",
    "    for file in folders:\n",
    "        !cp {file} {newfolders}\n",
    "    for filename in os.listdir(newfolders):\n",
    "        if filename != 'embarc_MG_subj_PRTsess_.txt' and filename != 'embarc_NY_subj_PRTsess_.txt'and filename != 'embarc_NY_subj****************_PRTsess2_short.txt' and filename != 'embarc_TX_subj_PRTsess_.txt' and filename != 'embarc_UM_subj_PRTsess_.txt' and filename !='Note.txt':\n",
    "            f = open(newfolders + filename, 'r', encoding='windows-1252')\n",
    "            for eachline in f:\n",
    "                string=re.sub(r'\\t',' ',eachline)\n",
    "                open('/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/' + state +'/Summaries/'+ filename,'a').write(string)\n",
    "                df_this = pd.read_csv('/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/' + state +'/Summaries/'+ filename, ' ', dtype={'subject':str})\n",
    "                dfall.append(df_this)\n",
    "    out = pd.concat(dfall)      \n",
    "    out.to_csv('/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/Standard' +state+ '.csv')\n",
    "\n",
    "    #-------------DELETE SOON------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'subject'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Users/mlm2/anaconda/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1944\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1945\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1946\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4154)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4018)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'subject'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8bbe96c1c927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelim_whitespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;31m#x['subject'] = '0' + x['subject']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dog'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mlm2/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1995\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1997\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mlm2/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2002\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2004\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mlm2/anaconda/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mlm2/anaconda/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3290\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3291\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3292\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mlm2/anaconda/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1945\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1946\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1947\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4154)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4018)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'subject'"
     ]
    }
   ],
   "source": [
    "# for all states extract the standard data and put into one file\n",
    "# TODO: fix weird IDs\n",
    "\n",
    "df_stand = pd.DataFrame()\n",
    "for state in ['Massachusetts', 'Michigan', 'New_York', 'Texas']:\n",
    "    df_state = pd.DataFrame()\n",
    "    statesum = '/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/'+ state +'/Summaries/embarc*/*txt'\n",
    "    folders  = !ls -d {statesum}\n",
    "    \n",
    "    for file in folders:\n",
    "        x = pd.read_csv(file, delim_whitespace = True, dtype={'subject':str})\n",
    "\n",
    "        df_stand = pd.concat([x, df_stand])\n",
    "        df_state = pd.concat([x, df_state])\n",
    "    df_state['ProjectSpecificID'] = df_state['site'] + df_state['subject'] #Need to fix wonky IDs in the same way as before\n",
    "    df_state = df_state[pd.notnull(df_state['subject'])]\n",
    "    df_state.to_csv('standard_' + state + '_' + today + '.csv')\n",
    "    \n",
    "df_stand = df_stand[pd.notnull(df_stand['subject'])]\n",
    "df_stand.to_csv('standard_all_sessions_all_states_' + today + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dan note:** I'm not sure why you're replacing tabs with spaces--is this a preamble to reading the files into a df? If so, it's not necessary as you can set delim_whitespace = True. If it's a question of extracting a subset of variables from these files, you can right a regex that looks for whitespace whether it's a tab or a space. Would either of those solutions work? They seem like they would be more streamlined and easier to manage.\n",
    "\n",
    "**VL:** delim_whitespace = True is 1000X easier, thanks!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is mostly right, just need to fix it to make it work with the above cell \n",
    "\n",
    "df_allStatesStandard = []\n",
    "\n",
    "for val in [('Massachusetts','MG'),('Michigan','UM'),('New_York','CU'),('Texas','TX')]:\n",
    "    state, abrev = val\n",
    "    standard_file = '/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/Standard'+ state +'.csv'\n",
    "    one_sess_file = '/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/'+ state +'_one_sess_stand.csv'\n",
    "    all_sess_file = '/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/'+ state +'_all_sess_stand.csv'\n",
    "    \n",
    "    # this is problematic, can only do once eek\n",
    "    #replacetabs(state)\n",
    "    \n",
    "    df_state = pd.read_csv(standard_file, dtype={'subject':str}) \n",
    "    df_state.subject = df_state.subject.str.zfill(4)\n",
    "    df_state['ProjectSpecificID'] =  abrev + df_state.subject\n",
    "    \n",
    "    df_group = pd.read_csv('groups.csv',encoding=\"latin-1\")\n",
    "    df_group = df_group[np.isfinite(df_group['feasibility'])]\n",
    "    df_group = df_group.loc[df_group['feasibility'] != 2]\n",
    "    \n",
    "    df_all = df_state.merge(df_group, how='outer')\n",
    "    df_all.loc[df_all['ProjectSpecificID'] == 'MG0020', 'session'] = 1 #recode this presumed typo\n",
    "    df_all = df_all.loc[df_all['PRT_QC_Status'] == 1]\n",
    "    df_all = df_all[np.isfinite(df_all['feasibility'])]\n",
    "    df_firstsess = df_all.drop_duplicates('ProjectSpecificID')\n",
    "    \n",
    "    if abrev == 'MG':\n",
    "        df_firstsess = df_firstsess.loc[df_all['session'] == 1]\n",
    "    else:\n",
    "        df_firstsess = df_firstsess.loc[df_all['session'] == '1']\n",
    "\n",
    "    # Comparing to Christian's list\n",
    "    df_christian = pd.read_csv('/Users/mlm2/Work/Expts/EMBARC/Data/christian_included.txt', header = None)\n",
    "    df_christian = df_christian.loc[df_christian[0].str.startswith(abrev)]\n",
    "    christian_list = df_christian[0]\n",
    "    # are we missing anything?\n",
    "    for subject in christian_list:\n",
    "        if subject in df_firstsess.ProjectSpecificID.unique():\n",
    "            pass\n",
    "        else:\n",
    "            print('Missing from ours: '+ subject)\n",
    "    # do we have anything extra?\n",
    "    for subject in df_firstsess.ProjectSpecificID:\n",
    "        if subject in christian_list.unique():\n",
    "            pass\n",
    "        else:\n",
    "            print('Not included in Christians: ' + subject)\n",
    "    if abrev == 'UM':\n",
    "        print (christian_list.nunique())\n",
    "    # Based on Christian's list, remove these people\n",
    "    df_firstsess = df_firstsess.loc[df_all['ProjectSpecificID'] != 'UM0102']\n",
    "    df_firstsess = df_firstsess.loc[df_all['ProjectSpecificID'] != 'CU0047']\n",
    "    \n",
    "    if abrev == 'MG':\n",
    "        df_allStatesStandard = df_firstsess\n",
    "    else:\n",
    "        frames = [df_allStatesStandard, df_firstsess]\n",
    "        standardfinal = pd.concat(frames)\n",
    "    \n",
    "    #save these bad boys\n",
    "    #df_all.to_csv(all_sess_file)\n",
    "    #df_firstsess.to_csv(one_sess_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Massachusetts descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_mg = pd.read_csv('/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/Massachusetts_one_sess_stand.csv')\n",
    "\n",
    "df_mg.loc[df_mg['feasibility'] == 1, 'feasibility'] = 'MDD'\n",
    "df_mg.loc[df_mg['feasibility'] == 3, 'feasibility'] = 'MDD'\n",
    "df_mg.loc[df_mg['feasibility'] == 5, 'feasibility'] = 'MDD'\n",
    "df_mg.loc[df_mg['feasibility'] == 4, 'feasibility'] = 'CTL'\n",
    "\n",
    "print (df_mg.RB_adjst_all_Blk2_minus_Blk1.describe())\n",
    "\n",
    "t = df_mg.groupby(['feasibility'])\n",
    "print (t.RB_adjst_all_Blk2_minus_Blk1.describe())\n",
    "\n",
    "sns.distplot(df_mg.RB_adjst_all_Blk2_minus_Blk1.dropna())\n",
    "\n",
    "group_fig = sns.FacetGrid(data=df_mg.dropna(axis=0,subset=['RB_adjst_all_Blk2_minus_Blk1']),col='feasibility',col_wrap=4)\n",
    "group_fig.map(sns.distplot,'RB_adjst_all_Blk2_minus_Blk1')\n",
    "\n",
    "%Rpush df_mg\n",
    "%R cat <- ezPlot(data = df_mg, dv = RB_adjst_all_Blk2_minus_Blk1, between = feasibility, x= feasibility, wid=ProjectSpecificID)\n",
    "%R print(cat)\n",
    "%R ay <- ezANOVA(data = df_mg, dv = RB_adjst_all_Blk2_minus_Blk1, between = feasibility, wid=ProjectSpecificID)\n",
    "%R print(ay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dan note:** Above looks good, sometimes illuminating to overlay the two groups' histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New York descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_cu = pd.read_csv('/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/New_York_one_sess_stand.csv')\n",
    "df_cu.loc[df_cu['feasibility'] == 1, 'feasibility'] = 'MDD'\n",
    "df_cu.loc[df_cu['feasibility'] == 3, 'feasibility'] = 'MDD'\n",
    "df_cu.loc[df_cu['feasibility'] == 5, 'feasibility'] = 'MDD'\n",
    "df_cu.loc[df_cu['feasibility'] == 4, 'feasibility'] = 'CTL'\n",
    "\n",
    "print (df_cu.RB_adjst_all_Blk2_minus_Blk1.describe())\n",
    "\n",
    "t = df_cu.groupby(['feasibility'])\n",
    "print (t.RB_adjst_all_Blk2_minus_Blk1.describe())\n",
    "\n",
    "sns.distplot(df_cu.RB_adjst_all_Blk2_minus_Blk1.dropna())\n",
    "\n",
    "group_fig = sns.FacetGrid(data=df_cu.dropna(axis=0,subset=['RB_adjst_all_Blk2_minus_Blk1']),col='feasibility',col_wrap=4)\n",
    "group_fig.map(sns.distplot,'RB_adjst_all_Blk2_minus_Blk1')\n",
    "\n",
    "%Rpush df_cu\n",
    "%R cat <- ezPlot(data = df_cu, dv = RB_adjst_all_Blk2_minus_Blk1, between = feasibility, x= feasibility, wid=ProjectSpecificID)\n",
    "%R print(cat)\n",
    "%R ay <- ezANOVA(data = df_cu, dv = RB_adjst_all_Blk2_minus_Blk1, between = feasibility, wid=ProjectSpecificID)\n",
    "%R print(ay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dan note:** That bell curve for everybody's data is so textbook, wow! So far looks like RB is lower in MDDs . . . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Michigan descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_um = pd.read_csv('/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/Michigan_one_sess_stand.csv')\n",
    "df_um.loc[df_um['feasibility'] == 1, 'feasibility'] = 'MDD'\n",
    "df_um.loc[df_um['feasibility'] == 3, 'feasibility'] = 'MDD'\n",
    "df_um.loc[df_um['feasibility'] == 5, 'feasibility'] = 'MDD'\n",
    "df_um.loc[df_um['feasibility'] == 4, 'feasibility'] = 'CTL'\n",
    "\n",
    "print (df_um.RB_adjst_all_Blk2_minus_Blk1.describe())\n",
    "\n",
    "t = df_um.groupby(['feasibility'])\n",
    "print (t.RB_adjst_all_Blk2_minus_Blk1.describe())\n",
    "\n",
    "sns.distplot(df_um.RB_adjst_all_Blk2_minus_Blk1.dropna())\n",
    "\n",
    "group_fig = sns.FacetGrid(data=df_um.dropna(axis=0,subset=['RB_adjst_all_Blk2_minus_Blk1']),col='feasibility',col_wrap=4)\n",
    "group_fig.map(sns.distplot,'RB_adjst_all_Blk2_minus_Blk1')\n",
    "\n",
    "%Rpush df_um\n",
    "%R cat <- ezPlot(data = df_um, dv = RB_adjst_all_Blk2_minus_Blk1, between = feasibility, x= feasibility, wid=ProjectSpecificID)\n",
    "%R print(cat)\n",
    "%R ay <- ezANOVA(data = df_um, dv = RB_adjst_all_Blk2_minus_Blk1, between = feasibility, wid=ProjectSpecificID)\n",
    "%R print(ay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texas descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_tx = pd.read_csv('/Users/'+ uname +'/Work/Expts/EMBARC/Data/PRT/Texas_one_sess_stand.csv')\n",
    "df_tx.loc[df_tx['feasibility'] == 1, 'feasibility'] = 'MDD'\n",
    "df_tx.loc[df_tx['feasibility'] == 3, 'feasibility'] = 'MDD'\n",
    "df_tx.loc[df_tx['feasibility'] == 5, 'feasibility'] = 'MDD'\n",
    "df_tx.loc[df_tx['feasibility'] == 4, 'feasibility'] = 'CTL'\n",
    "\n",
    "print (df_tx.RB_adjst_all_Blk2_minus_Blk1.describe())\n",
    "\n",
    "t = df_tx.groupby(['feasibility'])\n",
    "print (t.RB_adjst_all_Blk2_minus_Blk1.describe())\n",
    "\n",
    "sns.distplot(df_tx.RB_adjst_all_Blk2_minus_Blk1.dropna())\n",
    "\n",
    "group_fig = sns.FacetGrid(data=df_tx.dropna(axis=0,subset=['RB_adjst_all_Blk2_minus_Blk1']),col='feasibility',col_wrap=4)\n",
    "group_fig.map(sns.distplot,'RB_adjst_all_Blk2_minus_Blk1')\n",
    "\n",
    "%Rpush df_tx\n",
    "%R cat <- ezPlot(data = df_tx, dv = RB_adjst_all_Blk2_minus_Blk1, between = feasibility, x= feasibility, wid=ProjectSpecificID)\n",
    "%R print(cat)\n",
    "%R ay <- ezANOVA(data = df_tx, dv = RB_adjst_all_Blk2_minus_Blk1, between = feasibility, wid=ProjectSpecificID)\n",
    "%R print(ay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dan note:** Thanks for making a liar out of me, Michigan and Texas . . . Next step with this is to use factorplot to generate bar graphs with ci=68% for RB, with hue=group. Do that once collapsing across site, then do it with col=site (or row=site if that's better). Then if you're up for it, push the data to R and use ezANOVA to run a Group x Site ANOVA (wid=ProjectSpecificID) on the RB data. Everywhere I say \"RB\", I mean \"RB_adjst_all_Blk2_minus_Blk1\" (wipes sweat from brow due to over-exertion whilst typing long variable name . . . ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Starting the HDDM part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = hddm.load_csv('embarc_compact_' + today +'.csv')\n",
    "\n",
    "# rt needs to be in seconds in order to fit to a model\n",
    "data['rt'] = data['rt'] / 1000\n",
    "\n",
    "data #looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (hddm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#flip error RTs so they become negative\n",
    "data = hddm.utils.flip_errors(data)\n",
    "\n",
    "#plot the RTs and save them \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, xlabel='RT', ylabel='count', title='RT distributions')\n",
    "for i, subj_data in data.groupby('subj_idx'): \n",
    "    subj_data.rt.hist(bins=30, histtype='step', ax=ax)\n",
    "plt.savefig('RT_dist.pdf')\n",
    "\n",
    "# it looks like there's one person with too many trials, should be fixed when the earlier cell is rewritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creates a simple model that doesn't take conditions into account\n",
    "m=hddm.HDDM(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this part has to do with Markov-Chain Monte Carlo (MCMC; https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)\n",
    "# it seems like MCMC gets the distribution of the posterior probababilities, which seem like \n",
    "\n",
    "m.find_starting_values()\n",
    "m.sample(2000, burn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats = m.gen_stats()\n",
    "stats[stats.index.isin(['a', 'a_std', 'a_subj.MG0001', 'a_subj.MG0002'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#From Demo:\n",
    "\n",
    "# \"The inference algorithm, MCMC, requires the chains of the model to have properly converged. \n",
    "#While there is no way to guarantee convergence for a finite set of samples in MCMC, there are \n",
    "#many heuristics that allow you identify problems of convergence. One main analysis to look at \n",
    "#is the trace, the autocorrelation, and the marginal posterior. You can plot these using the \n",
    "#plot_posteriors() function.\"\n",
    "\n",
    "m.plot_posteriors(['a', 't', 'v', 'a_std'])\n",
    "\n",
    "#we are looking for drifts or large jumps, based off of the example they gave it looks like \n",
    "#we're good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TAKES FOREVER--don't do unless you need to. There might be something wrong here, it really \n",
    "# shouldn't take this long. \n",
    "\n",
    "# \"The Gelman-Rubin statistic provides a more formal test for convergence that compares the \n",
    "# intra-chain variance to the intra-chain variance of different runs of the same model.\"\n",
    "\n",
    "models = []\n",
    "\n",
    "for i in range(5):\n",
    "    m = hddm.HDDM(data)\n",
    "    m.find_starting_values()\n",
    "    m.sample(5000, burn = 20)\n",
    "    models.append(m)\n",
    "\n",
    "#Values should be close to 1, and they are. \n",
    "hddm.analyze.gelman_rubin(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for time's sake, I used 1000 & 100 to test, but it should really be 10000 & 1000\n",
    "m_stim = hddm.HDDM(data, depends_on={'v': 'length'})\n",
    "m_stim.find_starting_values()\n",
    "m_stim.sample(1000, burn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v_short, v_long = m_stim.nodes_db.node[['v(short)', 'v(long)']]\n",
    "hddm.analyze.plot_posterior_nodes([v_short, v_long])\n",
    "plt.xlabel('drift-rate')\n",
    "plt.ylabel('Posterior probability')\n",
    "plt.title('Posterior of drift-rate group means')\n",
    "plt.savefig('long_short_drift_rate.pdf')\n",
    "\n",
    "# think this makes sense because the short one is the rich stim always\n",
    "# we are treating short/long like a between subjects factor though, so we should redo it, not\n",
    "# sure how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for time's sake, I used 1000 & 100 to test, but it should really be 10000 & 1000\n",
    "\n",
    "m_stim = hddm.HDDM(data, depends_on={'v': 'feasibility'})\n",
    "m_stim.find_starting_values()\n",
    "m_stim.sample(1000, burn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v_MDD, v_CTL = m_stim.nodes_db.node[['v(MDD)','v(CTL)']]\n",
    "hddm.analyze.plot_posterior_nodes([v_MDD, v_CTL])\n",
    "plt.xlabel('drift-rate')\n",
    "plt.ylabel('Posterior probability')\n",
    "plt.title('Posterior of drift-rate group means')\n",
    "plt.savefig('long_short_drift_rate.pdf')\n",
    "\n",
    "#I think this will be cleaned up a little when we run it with a larger sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
